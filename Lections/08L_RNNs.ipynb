{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Existanze54/sirius-neural-networks-2024/blob/main/Lections/08L_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LTc4sTbNv9o"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM2Dsn4DNv_4"
      },
      "source": [
        "Рекуррентные нейронные сети $-$ подмножество нейронных сетей, разработанное для работы с неструктурированными данными, представляющими собой некие последовательности объектов, не имеющие связной структуры и признаковых описаний. К примерам таких аднных можно отнести звуковые ряды, видео, временные ряды и конечно же текст. За счет своего устройства нейронные сети способны выделять закономерности, содержащиеся в таких последовательностях, запоминать, и использовать эту информацию при дальнейшем анализе последовательности.\n",
        "\n",
        "Название \"рекуррентные\" проистекает из наличия в сети виртуального набора последовательных однотипных ячеек, через которые и пропускается информация. В реальности в нашей модели в рекуррентном слое такая ячейка только одна, однако при пропускании через себя последовательности она изменяет свое состояние и \"запоминает\" то, что видела ранее.\n",
        "\n",
        "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" alt=\"Drawing\" width= \"700px;\"/>\n",
        "<a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Source</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL2lgvISNwBp"
      },
      "source": [
        "\n",
        "Варианты архитектуры рекуррентный нейронных сетей не ограничиваются схемой \"один вход - один выход\", что используется, например, для классификации изображений. One-to-One -- схема, с которой мы работали ранее: принимаем на вход 1 объект, обрабатываем его полностью, выдаем одно предсказание (например, метку класса).<br>\n",
        "\n",
        "<img src=\"https://kodomo.fbb.msu.ru/FBB/year_20/ml/rnn/diags.jpeg\" alt=\"Drawing\" width= \"800px;\"/>\n",
        "\n",
        "Рекуррентные нейронные сети позволяют решать и другие задачи.\n",
        "Чуть более сложным вариантом является схема \"one_to-many\", подразумевающая генерацию последовательности объектов в ответ на один поданный на вход. Например, такую схему используют для генерации текстового описания картинки.<br>\n",
        "Существуют варианты и обратные предыдущему, например, для создания изображения по его текстовому описанию.<br>\n",
        "Наконец, возможны архитектуры вида \"many-to-many\", генерирующие последовательность в ответ на последовательность. Такие могут быть использованы для перевода текстов или генерации покадрового описания к видео. Такие архитектуры могут выдавать результат после полного поглощения исходной последовательности, для лучшего выделения контекста, или сразу во время чтения, например, когда нужен синхронный перевод текста.\n",
        "\n",
        "([Источник](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaxYMW0tNwD0"
      },
      "source": [
        "## Базовая ячейка RNN  (RNNCell)\n",
        "\n",
        "Для начала посмотрим на то, что происходит внутри одной рекуррентной ячейки. <br>\n",
        "В отличие от нейрона \"обычной\" сети, рекуррентная ячейка принимает два входа: элемент последовательности $X_t$ и предыдущее скрытое состояние $h_{t-1}$. Текущее скрытое состояние вычисляется при помощи взвешенного сложения входов, после чего может быть вычисле выход, а текущее скрытое состояние передается далее и может быть использовано для вычисления $h_{t+1}$.\n",
        "\n",
        "\n",
        "$$h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_t)$$\n",
        "\n",
        "\n",
        "$$y_t = W_{hy}h_t$$\n",
        "\n",
        "<img src=\"https://kodomo.fbb.msu.ru/FBB/year_20/ml/rnn/rnncell.png\" alt=\"Drawing\" width= \"500px;\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ6_8iIpNwH5"
      },
      "source": [
        "Важно понимать, что веса, использующиеся для получения скрытого состояния, во время предсказания остаются неизменными для всех элементов последовательности (и именно они ответственны за обучение RNN ячейки).\n",
        "\n",
        "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\" alt=\"Drawing\" width= \"700px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV_M5p71cgcJ"
      },
      "source": [
        "## Блок RNN\n",
        "\n",
        "В Pytorch реализованы готовые классы как для базовой рекуррентной ячейки RNNCell, так и для настоящего рекуррентного слоя под названием [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html). Этот класс освобождает нас от необходимости писать циклы и позволяет применить внутреннюю RNN ячейку ко всех элементам поданной на вход последовательности. При создании ячейки изначальный хидден стейт по дефолту заполняется нулями, однако можно инициализировать его любым тензором нужной формы.\n",
        "\n",
        "Важно обратить внимание на то, что по умолчанию батчевая размерность подразумевается второй по счету. Для управления этим поведением предусмотрен булевый аргумент **batch_first**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CMX25u6eIHE",
        "outputId": "b4af350f-4efe-45b2-c68c-31ea6bd0b59b"
      },
      "source": [
        "from torch import nn\n",
        "rnn = torch.nn.RNN(input_size = 3, hidden_size = 2) # batch_first = False\n",
        "dummy_batched_seq = torch.randn((2,1,3)) # seq_len, batch , input_size\n",
        "out, h = rnn(dummy_batched_seq)\n",
        "\n",
        "print(f\"Dummy: \\n {dummy_batched_seq.shape}\\n {dummy_batched_seq}\")\n",
        "print(f\"Out: \\n {out.shape}\\n {out}\") # hidden state for each element of sequence\n",
        "print(f\"h: \\n {h.shape}\\n {h}\") # hidden state for last element of sequence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dummy: \n",
            " torch.Size([2, 1, 3])\n",
            " tensor([[[-1.8667,  1.2638,  0.0826]],\n",
            "\n",
            "        [[ 0.1110, -0.5911, -0.8078]]])\n",
            "Out: \n",
            " torch.Size([2, 1, 2])\n",
            " tensor([[[-0.4575, -0.0697]],\n",
            "\n",
            "        [[-0.4038,  0.2472]]], grad_fn=<StackBackward>)\n",
            "h: \n",
            " torch.Size([1, 1, 2])\n",
            " tensor([[[-0.4038,  0.2472]]], grad_fn=<StackBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDROEUzecgiD"
      },
      "source": [
        "Попробуем написать простую рекуррентную нейросетку!<br>\n",
        "Добавим выходной полносвязный слой, принимающий на вход последнее скрытое состояние. <br>\n",
        "Обратите внимание на то, что из-за особенностей торча у выхода будет на одну размерность больше. Такая \"лишняя\" размерность может быть весьма надоедливой, и лучше её сразу убрать за ненадобностью."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVs2aG7cgNfm",
        "outputId": "62061a80-a70e-4fd7-df0a-3dbff9a26e56"
      },
      "source": [
        "import torch\n",
        "# Let's add output weights\n",
        "\n",
        "class RNN_for_many_to_one(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden, output_size):\n",
        "        super().__init__()\n",
        "        self.rnn = torch.nn.RNN(input_size, hidden, batch_first=True) # RNN слой\n",
        "        self.fc1 = torch.nn.Linear(hidden, output_size)                 # Полносвязный слой\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, hidden = self.rnn(x)\n",
        "        print(\"All hidden states:\\t\", x.shape) # h for each element\n",
        "        print(\"Last hidden state:\\t\", hidden.shape)\n",
        "        # we need only last output\n",
        "        # return self.fc1(x[-1])\n",
        "        return self.fc1(hidden)\n",
        "\n",
        "model2 = RNN_for_many_to_one(28, 128, 10) # input_size, hidden_dim, classes\n",
        "dummy_input = torch.randn((8, 28, 28)) #  batch, seq_len, element_size\n",
        "res = model2(dummy_input)\n",
        "\n",
        "print(\"Annoying extra dimension:\", res.shape)\n",
        "print(\"Real output:\\t\\t\", res[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All hidden states:\t torch.Size([8, 28, 128])\n",
            "Last hidden state:\t torch.Size([1, 8, 128])\n",
            "Annoying extra dimension: torch.Size([1, 8, 10])\n",
            "Real output:\t\t torch.Size([8, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XnTieY_j5Fo"
      },
      "source": [
        "## Bidirectional RNNs\n",
        "\n",
        "Последовательность можно пропустить через сеть два раза, в прямом и обратном направлении.\n",
        "\n",
        "Идея тут в том, что идя по предложению, мы не знаем что у него будет в конце. А иногда это нужно по постановке задачи (например, перевод). Таким образом, на каждом шаге будет собираться информация с начала и конца предложения ( и понемногу затухать). Иногда это работает лучше. Отвечает за такую архитектуру параметр **bidirectional**.\n",
        "\n",
        "<img src=\"https://kodomo.fbb.msu.ru/FBB/year_20/ml/rnn/bidirectional.png\" alt=\"Drawing\" width= \"700px;\"/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA8AR9fV7PnK",
        "outputId": "877a5fe6-4a28-4d50-bfd2-6b4123551b62"
      },
      "source": [
        "import torch\n",
        "\n",
        "dummy_input = torch.randn((2,1,3)) #   seq_len, batch, input_size\n",
        "rnn = torch.nn.RNN(3, 2, bidirectional=True, num_layers=1)\n",
        "\n",
        "for t, p in rnn.named_parameters():\n",
        "  print(t, p.shape)\n",
        "\n",
        "out, h = rnn(dummy_input)\n",
        "print(\"\")\n",
        "print(\"Out\", out) # Concatenated Hidden states from both layers\n",
        "print(\"h\", h) # Hidden states last element from  both : 2*num_layers*hidden_state"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weight_ih_l0 torch.Size([2, 3])\n",
            "weight_hh_l0 torch.Size([2, 2])\n",
            "bias_ih_l0 torch.Size([2])\n",
            "bias_hh_l0 torch.Size([2])\n",
            "weight_ih_l0_reverse torch.Size([2, 3])\n",
            "weight_hh_l0_reverse torch.Size([2, 2])\n",
            "bias_ih_l0_reverse torch.Size([2])\n",
            "bias_hh_l0_reverse torch.Size([2])\n",
            "\n",
            "Out tensor([[[-0.8525, -0.0962,  0.4507, -0.1198]],\n",
            "\n",
            "        [[ 0.3427,  0.1302,  0.4982, -0.2478]]], grad_fn=<CatBackward>)\n",
            "h tensor([[[ 0.3427,  0.1302]],\n",
            "\n",
            "        [[ 0.4507, -0.1198]]], grad_fn=<StackBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKk3qsSQj5IG"
      },
      "source": [
        "## LSTM\n",
        "\n",
        "В конце 90-х важным прорывом в нейросетевой науке было изобретение ячеек *long short-term memory*. Важным отличием от обычной RNN ячейки является наличие новой \"трассы\", в которой аккумулируется (и со временем \"забывается\") результат объединения всех предыдущих скрытых состояний. Таким образом, ячейка помимо собственно скрытого состояния обладает \"**памятью**\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37J3BzEDj5KO"
      },
      "source": [
        "<img src=\"https://kodomo.fbb.msu.ru/FBB/year_20/ml/rnn/lstm.png\" alt=\"Drawing\" width= \"700px;\"/>\n",
        "\n",
        "Рассмотрим элементы LSTM-ячейки по отдельности и разберемся по-подробнее с тем, что же здесь происходит. Уже упомянутая трасса памяти обозначена как горизонтальная линия, бегущая сквозным конвейером через ячейки:   \n",
        "\n",
        "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png\" alt=\"Drawing\" width= \"700px;\"/>\n",
        "\n",
        "Помимо этой трассы, в LSTM-ячейке реализованы дополнительные операции, отвечающие за добавление и удаление информации из нее. Количество информации, пропускаемое через эти дополнительные каналы, регулируется так называемыми \"шлюзами\" при помощи сигмоиды, которая и принимает решение,  какие значения оставить, а какие отбросить в каждом случае. Таких шлюзов реализовано 3:\n",
        "\n",
        "1. **Forget gate**, или Шлюз Забывания отвечает за удаление информации из трассы состояния ячейки. Информация, которая больше не требуется LSTM для понимания вещей, или менее важная информация удаляется путем умножения на фильтр - матрицу весов. Это необходимо для оптимизации производительности сети LSTM. На вход принимает предыдущее скрытое состояние $h_{t-1}$ и текущий элемент последовательности $x_t$. Заданные входные данные умножаются на матрицы весов и добавляется смещение. После применения сигмоиды полученный фильтр применяется к трассе памяти.\n",
        "\n",
        "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\" alt=\"Drawing\" width= \"700px;\"/>\n",
        "\n",
        "2. **Input gate** или входной шлюз. Похож на предыдущий шаг, но действует в обратном направлении: ячейка принимает решение о добавлении новой информации в трассу памяти. Это делается в два этапа: сначала при помощи сигмоиды принимается решение, какие именно значения мы обновим (получаем тензор индексов $i_t$), а при помощи тангенса мы получаем собственно новые значения $C_t$, пригодные к добавлению в трассу. Обратите внимание, что для этих шагов мы обучаем разные матрицы весов. Совместив два тензора на этом шаге мы готовы добавить новую инормацию в трассу.  \n",
        "\n",
        "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\" alt=\"Drawing\" width= \"700px;\"/>\n",
        "\n",
        "\n",
        "\n",
        "3. Выходной шлюз, или **Output gate** принимает решение о том, какой же хидден стейт ячейка вернет для текущего элемента последовательности $x_t$. Для этого мы отфильтруем его при помощи сигмоиды и после этого умножим на текущее состояние памяти ячейки, пропущенное черз тангенс (чтобы значения не выходили за пределы -1..1).\n",
        "\n",
        "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\" alt=\"Drawing\" width= \"700px;\"/>\n",
        "\n",
        "Следующее состояние ячейки примет на вход память, хидден стейт и следующий элемент последовательности.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgFWPfV2r6PC"
      },
      "source": [
        "## GRU\n",
        "\n",
        "GRU, или Gated Reccurent Unit, являются еще одной версией RNN с подобием памяти.\n",
        "\n",
        "У этой версии не три в хода, а два: нет \"трассы\" внутреннего состояния, отличного от скрытого. Вместо этого, два шлюза отвечают за обновление и забывание информации в скрытом состоянии, которое и передается дальше без дополнительного наложения нелинейности.\n",
        "\n",
        "<img src=\"https://kodomo.fbb.msu.ru/FBB/year_20/ml/rnn/gru.png\" alt=\"Drawing\" width= \"900px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VH5hb9vtCE1"
      },
      "source": [
        "## Как это применяется в биологии?\n",
        "\n",
        "Долгое время RNN были в биологии весьма сильным методом как раз из-за своей способности работать с биологическими последовательностями. Впрочем, даже тогда с ними достаточно сильно конкурировали сверточные сети.\n",
        "\n",
        "На сегодняшний день все еще можно использовать RNN для каких-то не сильно сложных задач, но нет никакой гарантии, что сверточная модель вас не обойдёт.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Всё же, \"в чистом виде\" модели, в которых RNN использовался бы в качестве входного блока, обычно достаточно неглубокие (1 слой RNN + 1 слой fc классификатор) и используются для каких-то сравнительно просто решаемых задач, либо в задачах text-майнинга.\n",
        "\n",
        "DOI: 10.1007/s00251-013-0720-y\n",
        "\n",
        "Предсказание связывания пептидов с MHC II\n",
        "\n",
        "\n",
        "<img src=\"https://kodomo.fbb.msu.ru/FBB/year_20/ml/rnn/example3.png\" alt=\"Drawing\" width= \"600px;\"/>"
      ],
      "metadata": {
        "id": "eYq6m4PX-nmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гораздо чаще встречаются архитектуры, в которых входные слои сверточные, позволяющие получить из one-hot закодированных последовательностей/выравниваний более \"плотные\" эмбеддинги, которые уже подаются на вход рекуррентным блокам, а их выход -- уже полносвязным слоям.\n",
        "\n",
        "\n",
        "Предсказание клеточной локализации белка на основе последовательности.\n",
        "\n",
        "doi: 10.1093/bioinformatics/btx531\n",
        "\n",
        "<img src=\"https://kodomo.fbb.msu.ru/FBB/year_20/ml/rnn/example4.png\" alt=\"Drawing\" width= \"600px;\"/>"
      ],
      "metadata": {
        "id": "ux2nGOAq_wo-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BayvrbEEqiul"
      },
      "source": [
        "Достаточно красивая статья. Пожалуй, одна из последних статей, когда \"чистым\" рекуррентным сетям ничто не угрожало на их поприще. Уже через год, в 2017, будут представлены первые трансформеры...\n",
        "\n",
        "\n",
        "<img src=\"https://kodomo.fbb.msu.ru/FBB/year_20/ml/rnn/dcrnn.png\" alt=\"Drawing\" width= \"900px;\"/>\n",
        "\n",
        "\n",
        "https://arxiv.org/abs/1604.07176 <br>\n",
        "https://github.com/icemansina/IJCAI2016\n",
        "\n",
        "\n",
        "В статье представлена сеть, предсказывающая вторичную структуру белка по последовательности для соревнования CASP (и на момент выхода превзошедшая всех конкурентов на то время). Принимала на вход эмбеддинг последовательностей и фичи из профиля пси-бласта, после конкатенации вход пропускается через свертки и отдается на растеразание двунаправленным GRUшкам (обоснование: поиск идет в два направления, потому что участие каждого остатка в структуре задается его двумя соседями с обеих сторон), на выходе полносвязные слои предсказывают вероятность каждого остатка принадлежать тому или иному элементу вторичной структуры и -- для валидации -- доступность для растворителя.\n",
        "\n",
        "Интересно, что для реализации модели авторы выбрали умиравший уже тогда фреймворк Theano."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jba-XKoUCBK5"
      },
      "source": [
        "## Чем всё закончилось?\n",
        "На следующей лекции вы поближе познакомитесь с трансформерами --  моделями, которые во многих задачах стали выступать лучше RNN как по эффективности, так и по времени работы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuAixv8DwrS1"
      },
      "source": [
        "\n",
        "\n",
        "<img src=\"https://kodomo.fbb.msu.ru/FBB/year_20/ml/rnn/trans1.png\" alt=\"Drawing\" width= \"500px;\"/>"
      ]
    }
  ]
}