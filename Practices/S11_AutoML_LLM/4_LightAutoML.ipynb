{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Existanze54/sirius-neural-networks-2024/blob/main/Practices/S11_AutoML_LLM/4_LightAutoML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Краткий обзор AutoML иструментов в 2024 году: <a href='https://habr.com/ru/articles/811425/'>link</a>"
      ],
      "metadata": {
        "id": "HMQY7_DXOWp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/Existanze54/sirius-neural-networks-2024/blob/main/Images/LAMA.png?raw=true\" width=600></img></center>"
      ],
      "metadata": {
        "id": "e_36YHBrWmP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LightAutoML github репозиторий: <a href='https://github.com/sb-ai-lab/LightAutoML'>link</a>"
      ],
      "metadata": {
        "id": "oQhBI2c0gQ1D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnLJDqxBcfA3"
      },
      "source": [
        "LightAutoML (LAMA) – мощный open-source AutoML фреймворк за которым стоит одна из сильнейших по экспертизе DS команд из Sber AI Lab. Суперсила LAMA – это бленды и настраиваемые эксперименты. В то же время LAMA скорее скальпель для профессионалов,. Давно не было обновления, очень надеюсь, что мы увидим его в ближайшее время."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightautoml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKF3XE4oA5k5",
        "outputId": "2e2ca891-a144-46ac-dc15-709373c2ea1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightautoml\n",
            "  Downloading lightautoml-0.3.8.1-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.4/416.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autowoe>=1.2 (from lightautoml)\n",
            "  Downloading AutoWoE-1.3.2-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.7/215.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting catboost>=0.26.1 (from lightautoml)\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes (from lightautoml)\n",
            "  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: holidays in /usr/local/lib/python3.10/dist-packages (from lightautoml) (0.48)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from lightautoml) (3.1.4)\n",
            "Collecting joblib<1.3.0 (from lightautoml)\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting json2html (from lightautoml)\n",
            "  Downloading json2html-1.3.0.tar.gz (7.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lightgbm<=3.2.1,>=2.3 (from lightautoml)\n",
            "  Downloading lightgbm-3.2.1-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from lightautoml) (3.3)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from lightautoml) (1.25.2)\n",
            "Collecting optuna (from lightautoml)\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas<2.0.0 (from lightautoml)\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting poetry-core<2.0.0,>=1.0.0 (from lightautoml)\n",
            "  Downloading poetry_core-1.9.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.5/309.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from lightautoml) (6.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from lightautoml) (1.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from lightautoml) (0.13.1)\n",
            "Collecting statsmodels<=0.14.0 (from lightautoml)\n",
            "  Downloading statsmodels-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch<=2.0.0,>=1.9.0 (from lightautoml)\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lightautoml) (4.66.4)\n",
            "Requirement already satisfied: StrEnum<0.5.0,>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from autowoe>=1.2->lightautoml) (0.4.15)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autowoe>=1.2->lightautoml) (3.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from autowoe>=1.2->lightautoml) (7.4.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from autowoe>=1.2->lightautoml) (2023.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from autowoe>=1.2->lightautoml) (1.11.4)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.10/dist-packages (from autowoe>=1.2->lightautoml) (5.0.2)\n",
            "Collecting sphinx-rtd-theme (from autowoe>=1.2->lightautoml)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost>=0.26.1->lightautoml) (0.20.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost>=0.26.1->lightautoml) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost>=0.26.1->lightautoml) (1.16.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from lightgbm<=3.2.1,>=2.3->lightautoml) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0->lightautoml) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->lightautoml) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels<=0.14.0->lightautoml) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels<=0.14.0->lightautoml) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<=2.0.0,>=1.9.0->lightautoml) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<=2.0.0,>=1.9.0->lightautoml) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<=2.0.0,>=1.9.0->lightautoml) (1.12)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<=2.0.0,>=1.9.0->lightautoml) (67.7.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<=2.0.0,>=1.9.0->lightautoml) (3.27.9)\n",
            "Collecting lit (from triton==2.0.0->torch<=2.0.0,>=1.9.0->lightautoml)\n",
            "  Downloading lit-18.1.6-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->lightautoml) (2.1.5)\n",
            "Collecting alembic>=1.5.0 (from optuna->lightautoml)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna->lightautoml)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna->lightautoml) (2.0.30)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna->lightautoml)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autowoe>=1.2->lightautoml) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autowoe>=1.2->lightautoml) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autowoe>=1.2->lightautoml) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autowoe>=1.2->lightautoml) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autowoe>=1.2->lightautoml) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autowoe>=1.2->lightautoml) (3.1.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna->lightautoml) (3.0.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost>=0.26.1->lightautoml) (8.3.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->autowoe>=1.2->lightautoml) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->autowoe>=1.2->lightautoml) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->autowoe>=1.2->lightautoml) (1.2.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->autowoe>=1.2->lightautoml) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (1.0.7)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (2.16.1)\n",
            "Requirement already satisfied: docutils<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (0.18.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (2.15.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx->autowoe>=1.2->lightautoml) (2.31.0)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->autowoe>=1.2->lightautoml)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<=2.0.0,>=1.9.0->lightautoml) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx->autowoe>=1.2->lightautoml) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx->autowoe>=1.2->lightautoml) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx->autowoe>=1.2->lightautoml) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx->autowoe>=1.2->lightautoml) (2024.2.2)\n",
            "Building wheels for collected packages: json2html\n",
            "  Building wheel for json2html (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for json2html: filename=json2html-1.3.0-py3-none-any.whl size=7593 sha256=f8e339d0287939633367e9330c5cb4ac086be67a6735f4867938d51fe65a9138\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/d8/b3/6f83a04ab0ec00e691de794d108286bb0f8bcdf4ade19afb57\n",
            "Successfully built json2html\n",
            "Installing collected packages: lit, json2html, poetry-core, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, Mako, joblib, colorlog, cmaes, pandas, nvidia-cusolver-cu11, nvidia-cudnn-cu11, alembic, statsmodels, sphinxcontrib-jquery, optuna, lightgbm, catboost, sphinx-rtd-theme, autowoe, triton, torch, lightautoml\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.14.2\n",
            "    Uninstalling statsmodels-0.14.2:\n",
            "      Successfully uninstalled statsmodels-0.14.2\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 4.1.0\n",
            "    Uninstalling lightgbm-4.1.0:\n",
            "      Successfully uninstalled lightgbm-4.1.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.5.3 which is incompatible.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.5 alembic-1.13.1 autowoe-1.3.2 catboost-1.2.5 cmaes-0.10.0 colorlog-6.8.2 joblib-1.2.0 json2html-1.3.0 lightautoml-0.3.8.1 lightgbm-3.2.1 lit-18.1.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 optuna-3.6.1 pandas-1.5.3 poetry-core-1.9.0 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1 statsmodels-0.14.0 torch-2.0.0 triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries & Set Parameters"
      ],
      "metadata": {
        "id": "D3xR3oyxWCbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard python libraries\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Essential DS libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "import torch\n",
        "\n",
        "# LightAutoML presets, task and report generation\n",
        "from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\n",
        "from lightautoml.tasks import Task\n",
        "from lightautoml.report.report_deco import ReportDeco"
      ],
      "metadata": {
        "id": "KhKf7XtlVX0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc52a1fa",
        "papermill": {
          "duration": 0.139087,
          "end_time": "2022-09-29T11:51:01.067902",
          "exception": false,
          "start_time": "2022-09-29T11:51:00.928815",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Here we setup some parameters to use in the kernel:\n",
        "- `N_THREADS` - number of vCPUs for LightAutoML model creation\n",
        "- `N_FOLDS` - number of folds in LightAutoML inner CV\n",
        "- `RANDOM_STATE` - random seed for better reproducibility\n",
        "- `TEST_SIZE` - houldout data part size\n",
        "- `TIMEOUT` - limit in seconds for model to train\n",
        "- `TARGET_NAME` - target column name in dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_THREADS = 4\n",
        "N_FOLDS = 5\n",
        "RANDOM_STATE = 42\n",
        "#TEST_SIZE = 0.2\n",
        "#TIMEOUT = 10*3600\n",
        "TIMEOUT = 5*60\n",
        "TARGET_NAME = 'hospital_death'"
      ],
      "metadata": {
        "id": "qCc4JJbSBfjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(RANDOM_STATE)\n",
        "torch.set_num_threads(N_THREADS)"
      ],
      "metadata": {
        "id": "pZClfxlMBibQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "G7NzwyQ9WFkh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fc3df57",
        "papermill": {
          "duration": 0.145882,
          "end_time": "2022-09-29T11:51:07.072793",
          "exception": false,
          "start_time": "2022-09-29T11:51:06.926911",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "It is important to note that missing values (NaN and other) in the data should be left as is, unless the reason for their presence or their specific meaning are known. Otherwise, AutoML model will perceive the filled NaNs as a true pattern between the data and the target variable, without knowledge and assumptions about missing values, which can negatively affect the model quality. LighAutoML can deal with missing values and outliers automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "2fgcyew_b-SR",
        "outputId": "8f7468f4-d55c-4dd6-90c5-4d9d1a39ed64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(44939, 83)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  hospital_id   age        bmi  elective_surgery  ethnicity  \\\n",
              "0           0        118.0  69.9  25.719814               0.0        2.0   \n",
              "1           1        185.0  57.0  20.357278               0.0        0.0   \n",
              "2           2         99.0  71.0  30.558683               0.0        5.0   \n",
              "3           3         21.0  75.0  44.990982               0.0        2.0   \n",
              "4           4         70.0  62.0  16.620499               0.0        0.0   \n",
              "\n",
              "   gender  height  icu_admit_source  icu_id  ...  cirrhosis  \\\n",
              "0     0.0   162.6               0.0   100.0  ...        0.0   \n",
              "1     1.0   182.9               0.0   687.0  ...        0.0   \n",
              "2     1.0   175.2               0.0   514.0  ...        0.0   \n",
              "3     1.0   175.2               3.0   504.0  ...        0.0   \n",
              "4     0.0   152.0               0.0   464.0  ...        1.0   \n",
              "\n",
              "   diabetes_mellitus  hepatic_failure  immunosuppression  leukemia  lymphoma  \\\n",
              "0                0.0              0.0                0.0       0.0       0.0   \n",
              "1                0.0              0.0                0.0       0.0       0.0   \n",
              "2                0.0              0.0                0.0       0.0       0.0   \n",
              "3                0.0              0.0                0.0       0.0       0.0   \n",
              "4                0.0              0.0                0.0       0.0       0.0   \n",
              "\n",
              "   solid_tumor_with_metastasis  apache_3j_bodysystem  apache_2_bodysystem  \\\n",
              "0                          0.0                   7.0                  4.0   \n",
              "1                          0.0                   9.0                  0.0   \n",
              "2                          0.0                   0.0                  0.0   \n",
              "3                          0.0                   7.0                  4.0   \n",
              "4                          0.0                   0.0                  0.0   \n",
              "\n",
              "   hospital_death  \n",
              "0               1  \n",
              "1               0  \n",
              "2               0  \n",
              "3               0  \n",
              "4               0  \n",
              "\n",
              "[5 rows x 83 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ee7a916-9348-4c39-97c4-27bce9cf9bce\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>hospital_id</th>\n",
              "      <th>age</th>\n",
              "      <th>bmi</th>\n",
              "      <th>elective_surgery</th>\n",
              "      <th>ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>height</th>\n",
              "      <th>icu_admit_source</th>\n",
              "      <th>icu_id</th>\n",
              "      <th>...</th>\n",
              "      <th>cirrhosis</th>\n",
              "      <th>diabetes_mellitus</th>\n",
              "      <th>hepatic_failure</th>\n",
              "      <th>immunosuppression</th>\n",
              "      <th>leukemia</th>\n",
              "      <th>lymphoma</th>\n",
              "      <th>solid_tumor_with_metastasis</th>\n",
              "      <th>apache_3j_bodysystem</th>\n",
              "      <th>apache_2_bodysystem</th>\n",
              "      <th>hospital_death</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>69.9</td>\n",
              "      <td>25.719814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>185.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>20.357278</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>182.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>687.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>99.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>30.558683</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>175.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>514.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>21.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>44.990982</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>175.2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>504.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>70.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>16.620499</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>464.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 83 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ee7a916-9348-4c39-97c4-27bce9cf9bce')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7ee7a916-9348-4c39-97c4-27bce9cf9bce button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7ee7a916-9348-4c39-97c4-27bce9cf9bce');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-28d60443-71d6-4a10-b465-7c1c5910f9fd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-28d60443-71d6-4a10-b465-7c1c5910f9fd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-28d60443-71d6-4a10-b465-7c1c5910f9fd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_df = pd.read_csv('https://raw.githubusercontent.com/Existanze54/sirius-neural-networks-2024/main/Datasets/patient-survival-prediction/train_preprocessed.csv')\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('https://raw.githubusercontent.com/Existanze54/sirius-neural-networks-2024/main/Datasets/patient-survival-prediction/test_preprocessed.csv')\n",
        "print(test_df.shape)\n",
        "test_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "ZRjaI6tYcx_f",
        "outputId": "581fa8c1-37c8-4b62-afb0-4ed0b1060d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19260, 83)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  hospital_id   age        bmi  elective_surgery  ethnicity  \\\n",
              "0           0        188.0  69.0  29.605976               0.0        2.0   \n",
              "1           1         10.0  68.0  27.986953               0.0        2.0   \n",
              "2           2        176.0  55.0  32.641470               1.0        2.0   \n",
              "3           3         19.0  53.0  19.444444               0.0        2.0   \n",
              "4           4        128.0  74.0  16.508909               0.0        2.0   \n",
              "\n",
              "   gender  height  icu_admit_source  icu_id  ...  cirrhosis  \\\n",
              "0     0.0   165.1               0.0   840.0  ...        0.0   \n",
              "1     1.0   185.4               0.0   428.0  ...        0.0   \n",
              "2     0.0   162.6               2.0   611.0  ...        0.0   \n",
              "3     1.0   180.0               3.0   653.0  ...        0.0   \n",
              "4     0.0   165.1               1.0   377.0  ...        0.0   \n",
              "\n",
              "   diabetes_mellitus  hepatic_failure  immunosuppression  leukemia  lymphoma  \\\n",
              "0                0.0              0.0                0.0       0.0       0.0   \n",
              "1                0.0              0.0                0.0       0.0       0.0   \n",
              "2                0.0              0.0                0.0       0.0       0.0   \n",
              "3                0.0              0.0                0.0       0.0       0.0   \n",
              "4                1.0              0.0                0.0       0.0       0.0   \n",
              "\n",
              "   solid_tumor_with_metastasis  apache_3j_bodysystem  apache_2_bodysystem  \\\n",
              "0                          0.0                   1.0                  1.0   \n",
              "1                          0.0                   7.0                  4.0   \n",
              "2                          0.0                   1.0                  1.0   \n",
              "3                          0.0                   1.0                  1.0   \n",
              "4                          0.0                   9.0                  0.0   \n",
              "\n",
              "   hospital_death  \n",
              "0               0  \n",
              "1               0  \n",
              "2               0  \n",
              "3               0  \n",
              "4               0  \n",
              "\n",
              "[5 rows x 83 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7ed6120-f3f3-467e-bfc8-af88cd404569\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>hospital_id</th>\n",
              "      <th>age</th>\n",
              "      <th>bmi</th>\n",
              "      <th>elective_surgery</th>\n",
              "      <th>ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>height</th>\n",
              "      <th>icu_admit_source</th>\n",
              "      <th>icu_id</th>\n",
              "      <th>...</th>\n",
              "      <th>cirrhosis</th>\n",
              "      <th>diabetes_mellitus</th>\n",
              "      <th>hepatic_failure</th>\n",
              "      <th>immunosuppression</th>\n",
              "      <th>leukemia</th>\n",
              "      <th>lymphoma</th>\n",
              "      <th>solid_tumor_with_metastasis</th>\n",
              "      <th>apache_3j_bodysystem</th>\n",
              "      <th>apache_2_bodysystem</th>\n",
              "      <th>hospital_death</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>29.605976</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>165.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>840.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>27.986953</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>185.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>428.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>176.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>32.641470</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>611.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>19.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>19.444444</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>653.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>128.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>16.508909</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>165.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 83 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7ed6120-f3f3-467e-bfc8-af88cd404569')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a7ed6120-f3f3-467e-bfc8-af88cd404569 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a7ed6120-f3f3-467e-bfc8-af88cd404569');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-98c6de5a-35c5-4ebe-b5d9-6219d399bda1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-98c6de5a-35c5-4ebe-b5d9-6219d399bda1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-98c6de5a-35c5-4ebe-b5d9-6219d399bda1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01fc1482",
        "papermill": {
          "duration": 0.143631,
          "end_time": "2022-09-29T11:51:07.448381",
          "exception": false,
          "start_time": "2022-09-29T11:51:07.304750",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Task definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0baa85a3",
        "papermill": {
          "duration": 0.141213,
          "end_time": "2022-09-29T11:51:08.027472",
          "exception": false,
          "start_time": "2022-09-29T11:51:07.886259",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "First we need to create ```Task``` object - the class to setup what task LightAutoML model should solve with specific loss and metric if necessary (more info can be found [here](https://lightautoml.readthedocs.io/en/latest/pages/modules/generated/lightautoml.tasks.base.Task.html#lightautoml.tasks.base.Task) in our documentation).\n",
        "\n",
        "The following task types are available:\n",
        "\n",
        "- ```'binary'``` - for binary classification.\n",
        "\n",
        "- ```'reg’``` - for regression.\n",
        "\n",
        "- ```‘multiclass’``` - for multiclass classification.\n",
        "\n",
        "- ```'multi:reg``` - for multiple regression.\n",
        "\n",
        "- ```'multilabel'``` - for multi-label classification.\n",
        "\n",
        "In this example we will consider a binary classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-29T11:51:08.317853Z",
          "iopub.status.busy": "2022-09-29T11:51:08.316560Z",
          "iopub.status.idle": "2022-09-29T11:51:08.329836Z",
          "shell.execute_reply": "2022-09-29T11:51:08.328301Z"
        },
        "id": "318fd77c",
        "papermill": {
          "duration": 0.162134,
          "end_time": "2022-09-29T11:51:08.332800",
          "exception": false,
          "start_time": "2022-09-29T11:51:08.170666",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "task = Task('binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "496a501f",
        "papermill": {
          "duration": 0.141426,
          "end_time": "2022-09-29T11:51:08.614167",
          "exception": false,
          "start_time": "2022-09-29T11:51:08.472741",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Note that only logloss loss is available for binary task and it is the default loss. Default metric for binary classification is ROC-AUC. See more info about available and default losses and metrics [here](https://lightautoml.readthedocs.io/en/latest/pages/modules/generated/lightautoml.tasks.base.Task.html#lightautoml.tasks.base.Task).\n",
        "\n",
        "**Depending on the task, you can and shold choose exactly those metrics and losses that you want and need to optimize.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be20e524",
        "papermill": {
          "duration": 0.146909,
          "end_time": "2022-09-29T11:51:09.195614",
          "exception": false,
          "start_time": "2022-09-29T11:51:09.048705",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "To solve the task, we need to setup columns roles. LightAutoML can automatically define types and roles of data columns, but it is possible to specify it directly through the dictionary parameter ```roles``` when training AutoML model (see next section \"AutoML training\"). Specific roles can be specified using a string with the name (any role can be set like this).  So the key in dictionary must be the name of the role, the value must be a list of the names of the corresponding columns in dataset. The **only role you must setup is** ```'target'``` **role** (that is column with target variable obviously), everything else (```'drop', 'numeric', 'categorical', 'group', 'weights'``` etc) is up to user:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-29T11:51:09.494961Z",
          "iopub.status.busy": "2022-09-29T11:51:09.494489Z",
          "iopub.status.idle": "2022-09-29T11:51:09.499640Z",
          "shell.execute_reply": "2022-09-29T11:51:09.498694Z"
        },
        "id": "efa2ce81",
        "papermill": {
          "duration": 0.160808,
          "end_time": "2022-09-29T11:51:09.501875",
          "exception": false,
          "start_time": "2022-09-29T11:51:09.341067",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "roles = {\n",
        "    'target': TARGET_NAME,\n",
        "    'drop': ['patient_id', 'encounter_id']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89adfd64",
        "papermill": {
          "duration": 0.143798,
          "end_time": "2022-09-29T11:51:09.793429",
          "exception": false,
          "start_time": "2022-09-29T11:51:09.649631",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "You can also optionally specify the following roles:\n",
        "\n",
        "- ```'numeric'``` - numerical feature\n",
        "\n",
        "- ```'category'``` - categorical feature\n",
        "\n",
        "- ```'text'``` - text data\n",
        "\n",
        "- ```'datetime'``` - features with date and time\n",
        "\n",
        "- ```'date'``` - features with date only\n",
        "\n",
        "- ```'group'``` - features by which the data can be divided into groups and which can be taken into account for group k-fold validation (so the same group is not represented in both testing and training sets)\n",
        "\n",
        "- ```'drop'``` - features to drop, they will not be used in model building\n",
        "\n",
        "- ```'weights'``` - object weights for the loss and metric\n",
        "\n",
        "- ```'path'``` - image file paths (for CV tasks)\n",
        "\n",
        "- ```'treatment'``` - object group in uplift modelling tasks: treatment or control\n",
        "\n",
        "Note that role name can be written in any case. Also it is possible to pass individual objects of role classes with specific arguments instead of strings with role names for specific tasks and more optimal pipeline construction ([more details](https://github.com/sb-ai-lab/LightAutoML/blob/master/lightautoml/dataset/roles.py)).\n",
        "\n",
        "For example, to set the date role, you can use the ```DatetimeRole``` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-29T11:51:10.087554Z",
          "iopub.status.busy": "2022-09-29T11:51:10.086418Z",
          "iopub.status.idle": "2022-09-29T11:51:10.092830Z",
          "shell.execute_reply": "2022-09-29T11:51:10.091594Z"
        },
        "id": "fed1c797",
        "papermill": {
          "duration": 0.157321,
          "end_time": "2022-09-29T11:51:10.095402",
          "exception": false,
          "start_time": "2022-09-29T11:51:09.938081",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# from lightautoml.dataset.roles import DatetimeRole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7ee2ed5",
        "papermill": {
          "duration": 0.13946,
          "end_time": "2022-09-29T11:51:10.379948",
          "exception": false,
          "start_time": "2022-09-29T11:51:10.240488",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Different seasonality can be extracted from the data through the ```seasonality``` parameter: years (```'y'```), months (```'m'```), days (```'d'```), weekdays (```'wd'```), hours (```'hour'```), minutes (```'min'```), seconds (```'sec'```), milliseconds (```'ms'```), nanoseconds (```'ns'```). This features will be considered as categorical. Another important parameter is ```base_date```. It allows to specify the base date and convert the feature to the distances to this date (set to ```False``` by default). Also for all roles classes there is a ```force_input``` parameter, and if it is ```True```, then the corresponding features will pass all further feature selections and won't be excluded (equals ```False``` by default). Also it is always possible to specify data type for all roles using ```dtype``` argument.\n",
        "\n",
        "Here is an example of such a role assignment through a class object for date feature (but there is no such feature in the considered dataset):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-29T11:51:10.725980Z",
          "iopub.status.busy": "2022-09-29T11:51:10.725570Z",
          "iopub.status.idle": "2022-09-29T11:51:10.730475Z",
          "shell.execute_reply": "2022-09-29T11:51:10.729031Z"
        },
        "id": "d01b3c1c",
        "papermill": {
          "duration": 0.146265,
          "end_time": "2022-09-29T11:51:10.733102",
          "exception": false,
          "start_time": "2022-09-29T11:51:10.586837",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# roles = {\n",
        "#     DatetimeRole(base_date=False, seasonality=('d', 'wd', 'hour')): 'date_time'\n",
        "# }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a57086b3",
        "papermill": {
          "duration": 0.13897,
          "end_time": "2022-09-29T11:51:11.568495",
          "exception": false,
          "start_time": "2022-09-29T11:51:11.429525",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Next we are going to create LightAutoML model with `TabularAutoML` class - preset with default model structure in just several lines.\n",
        "\n",
        "Let's discuss some of the params we can setup:\n",
        "- `task` - the type of the ML task (the only **must have** parameter)\n",
        "- `timeout` - time limit in seconds for model to train\n",
        "- `cpu_limit` - vCPU count for model to use\n",
        "- `reader_params` - parameter change for ```Reader``` object inside preset, which works on the first step of data preparation: automatic feature typization, preliminary almost-constant features, correct CV setup etc. For example, we setup `n_jobs` threads for typization algo, `cv` folds and `random_state` as inside CV seed.\n",
        "- `general_params` - general parameters dictionary, in which it is possible to specify a list of algorithms used (```'use_algos'```), nested CV using (```'nested_cv'```) etc.\n",
        "\n",
        "**Important note**: `reader_params` key is one of the YAML config keys, which is used inside `TabularAutoML` preset. [More details](https://github.com/sb-ai-lab/LightAutoML/blob/master/lightautoml/automl/presets/tabular_config.yml) on its structure with explanation comments can be found on the link attached. Each key from this config can be modified with user settings during preset object initialization. To get more info about different parameters setting (for example, ML algos which can be used in `general_params->use_algos`) please take a look at our [article on TowardsDataScience](https://towardsdatascience.com/lightautoml-preset-usage-tutorial-2cce7da6f936).\n",
        "\n",
        "Moreover, to receive the automatic report for our model we will use `ReportDeco` decorator and work with the decorated version in the same way as we do with usual one."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "automl = TabularUtilizedAutoML(\n",
        "    task = task,\n",
        "    timeout = TIMEOUT,\n",
        "    cpu_limit = N_THREADS,\n",
        "    tuning_params = {'max_tuning_time': 900},\n",
        "    reader_params = {'n_jobs': N_THREADS}\n",
        ")"
      ],
      "metadata": {
        "id": "JPHz4d4dCYVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c268223e",
        "papermill": {
          "duration": 0.139159,
          "end_time": "2022-09-29T11:51:12.437948",
          "exception": false,
          "start_time": "2022-09-29T11:51:12.298789",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "To run autoML training use ```fit_predict``` method.\n",
        "\n",
        "Main arguments:\n",
        "\n",
        "- `train_data` - dataset to train.\n",
        "- `roles` - column roles dict.\n",
        "- `verbose` - controls the verbosity: the higher, the more messages:\n",
        "        <1  : messages are not displayed;\n",
        "        >=1 : the computation process for layers is displayed;\n",
        "        >=2 : the information about folds processing is also displayed;\n",
        "        >=3 : the hyperparameters optimization process is also displayed;\n",
        "        >=4 : the training process for every algorithm is displayed;\n",
        "\n",
        "Note: out-of-fold prediction is calculated during training and returned from the fit_predict method"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "oof_pred = automl.fit_predict(train_df, roles = roles, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMitpGSqC_7p",
        "outputId": "0f586bbf-1157-4f1d-b053-d701df71054b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] Start automl \u001b[1mutilizator\u001b[0m with listed constraints:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.addons.utilization.utilization:Start automl \u001b[1mutilizator\u001b[0m with listed constraints:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] - time: 300.00 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.addons.utilization.utilization:- time: 300.00 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] - CPU: 4 cores\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.addons.utilization.utilization:- CPU: 4 cores\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] - memory: 16 GB\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.addons.utilization.utilization:- memory: 16 GB\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] \u001b[1mIf one preset completes earlier, next preset configuration will be started\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.addons.utilization.utilization:\u001b[1mIf one preset completes earlier, next preset configuration will be started\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] ==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.addons.utilization.utilization:==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] Start 0 automl preset configuration:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.addons.utilization.utilization:Start 0 automl preset configuration:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] \u001b[1mconf_0_sel_type_0.yml\u001b[0m, random state: {'reader_params': {'random_state': 42}, 'nn_params': {'random_state': 42}, 'general_params': {'return_all_predictions': False}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.addons.utilization.utilization:\u001b[1mconf_0_sel_type_0.yml\u001b[0m, random state: {'reader_params': {'random_state': 42}, 'nn_params': {'random_state': 42}, 'general_params': {'return_all_predictions': False}}\n",
            "INFO3:lightautoml.addons.utilization.utilization:Found reader_params in kwargs, need to combine\n",
            "INFO3:lightautoml.addons.utilization.utilization:Merged variant for reader_params = {'n_jobs': 4, 'random_state': 42}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] Stdout logging level is INFO.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.presets.base:Stdout logging level is INFO.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] Task: binary\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.presets.base:Task: binary\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] Start automl preset with listed constraints:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.presets.base:Start automl preset with listed constraints:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] - time: 299.99 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.presets.base:- time: 299.99 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] - CPU: 4 cores\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.presets.base:- CPU: 4 cores\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] - memory: 16 GB\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.presets.base:- memory: 16 GB\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:42] \u001b[1mTrain data shape: (44939, 83)\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.reader.base:\u001b[1mTrain data shape: (44939, 83)\u001b[0m\n",
            "\n",
            "INFO3:lightautoml.reader.base:Feats was rejected during automatic roles guess: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:17:58] Layer \u001b[1m1\u001b[0m train process start. Time left 283.92 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.base:Layer \u001b[1m1\u001b[0m train process start. Time left 283.92 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:18:02] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'tol': 1e-06, 'max_iter': 100, 'cs': [1e-05, 5e-05, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000], 'early_stopping': 2, 'categorical_idx': [95, 96, 97, 98], 'embed_sizes': array([11, 11, 11,  5], dtype=int32), 'data_size': 99}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 1e-05 score = 0.8570695569050832\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 5e-05 score = 0.8651004179787073\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.0001 score = 0.8686367583845654\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.0005 score = 0.8748398997850754\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.001 score = 0.8761253873644225\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.005 score = 0.8772265644524417\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.01 score = 0.8771731456490228\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.05 score = 0.8772014170040487\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 1e-05 score = 0.861626087119508\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 5e-05 score = 0.8719242015294647\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.0001 score = 0.8765790567051532\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.0005 score = 0.8852004923276854\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.001 score = 0.8870768668466038\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.005 score = 0.8883964050082471\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.01 score = 0.8884921527465386\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.05 score = 0.8882014732343679\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.1 score = 0.8882014732343679\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 1e-05 score = 0.8580726433248362\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 5e-05 score = 0.8678506272804518\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.0001 score = 0.8725230544309491\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.0005 score = 0.8808712263207878\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.001 score = 0.8824747275953415\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.005 score = 0.8837414717348928\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.01 score = 0.883793016194332\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.05 score = 0.8836494726845605\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.1 score = 0.8836494726845605\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 1e-05 score = 0.8583439546158845\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 5e-05 score = 0.8661666073124407\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.0001 score = 0.8691104988254111\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.0005 score = 0.8743472597091019\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.001 score = 0.8751929012345678\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.005 score = 0.8756635177687808\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.01 score = 0.8755120082970961\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.05 score = 0.8751162093267358\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 1e-05 score = 0.8586495969992017\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 5e-05 score = 0.8683957165056414\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.0001 score = 0.872675254049401\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.0005 score = 0.8804357719823735\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.001 score = 0.8822194665275368\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.005 score = 0.8835819088800306\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.01 score = 0.8836011455682424\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.05 score = 0.8832628613682257\n",
            "INFO3:lightautoml.ml_algo.torch_based.linear_model:Linear model: C = 0.1 score = 0.8832628613682257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:18:19] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.8817612834449979\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.8817612834449979\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:18:19] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:18:19] Time left 262.78 secs\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.base:Time left 262.78 secs\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:18:22] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task': 'train', 'learning_rate': 0.03, 'num_leaves': 32, 'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'num_threads': 2, 'max_bin': 255, 'min_data_in_bin': 3, 'num_trees': 1200, 'early_stopping_rounds': 200, 'random_state': 42}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 200 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.885849\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.888384\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[300]\tvalid's auc: 0.888928\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[400]\tvalid's auc: 0.88922\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[500]\tvalid's auc: 0.88909\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[356]\tvalid's auc: 0.889709\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 200 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.891572\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.895802\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[300]\tvalid's auc: 0.896463\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[400]\tvalid's auc: 0.89731\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[500]\tvalid's auc: 0.898051\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[600]\tvalid's auc: 0.897463\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[700]\tvalid's auc: 0.897066\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[500]\tvalid's auc: 0.898051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:18:51] Time limit exceeded after calculating fold 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.base:Time limit exceeded after calculating fold 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:18:51] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8938905757984704\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8938905757984704\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:18:51] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:18:51] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 1.00 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.tuning.optuna:Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 1.00 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:18:51] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lightautoml.utils.timer:Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
            "INFO:optuna.storages._in_memory:A new study created in memory with name: no-name-506d1f68-f9e8-44b9-a09b-d22c359cc9eb\n",
            "INFO3:lightautoml.ml_algo.boost_lgbm:Training until validation scores don't improve for 200 rounds\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[100]\tvalid's auc: 0.883725\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[200]\tvalid's auc: 0.885732\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[300]\tvalid's auc: 0.885442\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[400]\tvalid's auc: 0.885301\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:[500]\tvalid's auc: 0.885009\n",
            "DEBUG:lightautoml.ml_algo.boost_lgbm:Early stopping, best iteration is:\n",
            "[356]\tvalid's auc: 0.886118\n",
            "INFO:optuna.study.study:Trial 0 finished with value: 0.8861184522916979 and parameters: {'feature_fraction': 0.6872700594236812, 'num_leaves': 244, 'bagging_fraction': 0.8659969709057025, 'min_sum_hessian_in_leaf': 0.24810409748678125, 'reg_alpha': 2.5361081166471375e-07, 'reg_lambda': 2.5348407664333426e-07}. Best is trial 0 with value: 0.8861184522916979.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 1\u001b[0m with hyperparameters {'feature_fraction': 0.6872700594236812, 'num_leaves': 244, 'bagging_fraction': 0.8659969709057025, 'min_sum_hessian_in_leaf': 0.24810409748678125, 'reg_alpha': 2.5361081166471375e-07, 'reg_lambda': 2.5348407664333426e-07} scored 0.8861184522916979 in 0:00:54.383663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:19:46] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.tuning.optuna:Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
            "INFO2:lightautoml.ml_algo.tuning.optuna:The set of hyperparameters \u001b[1m{'feature_fraction': 0.6872700594236812, 'num_leaves': 244, 'bagging_fraction': 0.8659969709057025, 'min_sum_hessian_in_leaf': 0.24810409748678125, 'reg_alpha': 2.5361081166471375e-07, 'reg_lambda': 2.5348407664333426e-07}\u001b[0m\n",
            " achieve 0.8861 auc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:19:46] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.base:Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
            "DEBUG:lightautoml.ml_algo.base:Training params: {'task_type': 'CPU', 'thread_count': 2, 'random_seed': 42, 'num_trees': 5000, 'learning_rate': 0.03, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'boost_from_average': True, 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': 100, 'allow_writing_files': False}\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_cb:0:\ttest: 0.7628845\tbest: 0.7628845 (0)\ttotal: 67.5ms\tremaining: 5m 37s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:100:\ttest: 0.8753086\tbest: 0.8753086 (100)\ttotal: 1.57s\tremaining: 1m 16s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:200:\ttest: 0.8813265\tbest: 0.8813265 (200)\ttotal: 3.07s\tremaining: 1m 13s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:300:\ttest: 0.8836979\tbest: 0.8836979 (300)\ttotal: 4.55s\tremaining: 1m 11s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:400:\ttest: 0.8851432\tbest: 0.8851432 (400)\ttotal: 6.02s\tremaining: 1m 9s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:500:\ttest: 0.8859783\tbest: 0.8861164 (492)\ttotal: 8.07s\tremaining: 1m 12s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:600:\ttest: 0.8866423\tbest: 0.8866481 (593)\ttotal: 11.3s\tremaining: 1m 22s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:700:\ttest: 0.8865902\tbest: 0.8869249 (654)\ttotal: 12.9s\tremaining: 1m 19s\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Stopped by overfitting detector  (100 iterations wait)\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestTest = 0.8869248888\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestIteration = 654\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Shrink model to first 655 iterations.\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_cb:0:\ttest: 0.7760632\tbest: 0.7760632 (0)\ttotal: 15ms\tremaining: 1m 14s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:100:\ttest: 0.8870434\tbest: 0.8870434 (100)\ttotal: 1.53s\tremaining: 1m 14s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:200:\ttest: 0.8930534\tbest: 0.8930534 (200)\ttotal: 3.04s\tremaining: 1m 12s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:300:\ttest: 0.8953504\tbest: 0.8953504 (300)\ttotal: 4.54s\tremaining: 1m 10s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:400:\ttest: 0.8970726\tbest: 0.8970760 (397)\ttotal: 6s\tremaining: 1m 8s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:500:\ttest: 0.8980101\tbest: 0.8980101 (500)\ttotal: 7.5s\tremaining: 1m 7s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:600:\ttest: 0.8989216\tbest: 0.8989440 (599)\ttotal: 10.8s\tremaining: 1m 19s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:700:\ttest: 0.8994405\tbest: 0.8994405 (700)\ttotal: 13.3s\tremaining: 1m 21s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:800:\ttest: 0.8998563\tbest: 0.8999450 (787)\ttotal: 14.8s\tremaining: 1m 17s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:900:\ttest: 0.9000236\tbest: 0.9001043 (880)\ttotal: 16.3s\tremaining: 1m 14s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:1000:\ttest: 0.8999761\tbest: 0.9001339 (935)\ttotal: 17.8s\tremaining: 1m 11s\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Stopped by overfitting detector  (100 iterations wait)\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestTest = 0.9001338594\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestIteration = 935\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Shrink model to first 936 iterations.\n",
            "INFO2:lightautoml.ml_algo.base:===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m =====\n",
            "INFO3:lightautoml.ml_algo.boost_cb:0:\ttest: 0.7697582\tbest: 0.7697582 (0)\ttotal: 15.3ms\tremaining: 1m 16s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:100:\ttest: 0.8756085\tbest: 0.8756085 (100)\ttotal: 1.51s\tremaining: 1m 13s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:200:\ttest: 0.8799397\tbest: 0.8799397 (200)\ttotal: 3.01s\tremaining: 1m 11s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:300:\ttest: 0.8829069\tbest: 0.8829289 (293)\ttotal: 5.45s\tremaining: 1m 25s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:400:\ttest: 0.8848284\tbest: 0.8848284 (400)\ttotal: 8.77s\tremaining: 1m 40s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:500:\ttest: 0.8857469\tbest: 0.8857469 (500)\ttotal: 10.2s\tremaining: 1m 31s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:600:\ttest: 0.8868263\tbest: 0.8868474 (585)\ttotal: 11.7s\tremaining: 1m 25s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:700:\ttest: 0.8873372\tbest: 0.8873690 (695)\ttotal: 13.2s\tremaining: 1m 21s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:800:\ttest: 0.8877154\tbest: 0.8877604 (783)\ttotal: 14.7s\tremaining: 1m 17s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:900:\ttest: 0.8878694\tbest: 0.8879923 (844)\ttotal: 16.2s\tremaining: 1m 13s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:1000:\ttest: 0.8882247\tbest: 0.8882641 (965)\ttotal: 17.7s\tremaining: 1m 10s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:1100:\ttest: 0.8883925\tbest: 0.8886301 (1087)\ttotal: 20s\tremaining: 1m 10s\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Stopped by overfitting detector  (100 iterations wait)\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestTest = 0.8886300732\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestIteration = 1087\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Shrink model to first 1088 iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:20:42] Time limit exceeded after calculating fold 2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.base:Time limit exceeded after calculating fold 2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:20:42] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8919156073152174\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.base:Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8919156073152174\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:20:42] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.base:\u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:20:42] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 1.00 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.tuning.optuna:Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 1.00 secs\n",
            "INFO:optuna.storages._in_memory:A new study created in memory with name: no-name-9fcb5525-1e93-4bc4-b32e-179cdf5648e2\n",
            "INFO3:lightautoml.ml_algo.boost_cb:0:\ttest: 0.7481555\tbest: 0.7481555 (0)\ttotal: 13.8ms\tremaining: 1m 8s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:100:\ttest: 0.8745042\tbest: 0.8745042 (100)\ttotal: 1.34s\tremaining: 1m 4s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:200:\ttest: 0.8804225\tbest: 0.8804225 (200)\ttotal: 2.66s\tremaining: 1m 3s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:300:\ttest: 0.8836618\tbest: 0.8837374 (296)\ttotal: 4.03s\tremaining: 1m 2s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:400:\ttest: 0.8851430\tbest: 0.8851430 (400)\ttotal: 5.35s\tremaining: 1m 1s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:500:\ttest: 0.8862520\tbest: 0.8862545 (499)\ttotal: 6.67s\tremaining: 59.9s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:600:\ttest: 0.8873430\tbest: 0.8873430 (600)\ttotal: 7.99s\tremaining: 58.5s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:700:\ttest: 0.8879300\tbest: 0.8879566 (695)\ttotal: 9.29s\tremaining: 56.9s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:800:\ttest: 0.8883385\tbest: 0.8883503 (799)\ttotal: 11.9s\tremaining: 1m 2s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:900:\ttest: 0.8885181\tbest: 0.8885237 (874)\ttotal: 14.7s\tremaining: 1m 7s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:1000:\ttest: 0.8886996\tbest: 0.8887628 (991)\ttotal: 16.1s\tremaining: 1m 4s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:1100:\ttest: 0.8890252\tbest: 0.8891218 (1074)\ttotal: 17.4s\tremaining: 1m 1s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:1200:\ttest: 0.8893089\tbest: 0.8893275 (1193)\ttotal: 18.7s\tremaining: 59.2s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:1300:\ttest: 0.8894615\tbest: 0.8894673 (1297)\ttotal: 20s\tremaining: 57s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:1400:\ttest: 0.8894996\tbest: 0.8895180 (1395)\ttotal: 21.4s\tremaining: 54.9s\n",
            "DEBUG:lightautoml.ml_algo.boost_cb:1500:\ttest: 0.8894565\tbest: 0.8895252 (1412)\ttotal: 22.7s\tremaining: 52.8s\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Stopped by overfitting detector  (100 iterations wait)\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestTest = 0.8895252287\n",
            "INFO3:lightautoml.ml_algo.boost_cb:bestIteration = 1412\n",
            "INFO3:lightautoml.ml_algo.boost_cb:Shrink model to first 1413 iterations.\n",
            "INFO:optuna.study.study:Trial 0 finished with value: 0.8895252286699655 and parameters: {'max_depth': 4, 'l2_leaf_reg': 3.6010467344475403, 'min_data_in_leaf': 15}. Best is trial 0 with value: 0.8895252286699655.\n",
            "INFO3:lightautoml.ml_algo.tuning.optuna:\u001b[1mTrial 1\u001b[0m with hyperparameters {'max_depth': 4, 'l2_leaf_reg': 3.6010467344475403, 'min_data_in_leaf': 15} scored 0.8895252286699655 in 0:00:23.665454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:21:06] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.ml_algo.tuning.optuna:Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
            "INFO2:lightautoml.ml_algo.tuning.optuna:The set of hyperparameters \u001b[1m{'max_depth': 4, 'l2_leaf_reg': 3.6010467344475403, 'min_data_in_leaf': 15}\u001b[0m\n",
            " achieve 0.8895 auc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:21:06] Time left 96.20 secs\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.base:Time left 96.20 secs\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:21:06] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.base:Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:21:06] \u001b[1mLayer 1 training completed.\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.base:\u001b[1mLayer 1 training completed.\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:21:06] Blending: optimization starts with equal weights and score \u001b[1m0.8878270225483917\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.blend:Blending: optimization starts with equal weights and score \u001b[1m0.8878270225483917\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:21:06] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8933582621082622\u001b[0m, weights = \u001b[1m[0.  0.5 0.5]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.blend:Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8933582621082622\u001b[0m, weights = \u001b[1m[0.  0.5 0.5]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:21:06] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8933582621082622\u001b[0m, weights = \u001b[1m[0.  0.5 0.5]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.blend:Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8933582621082622\u001b[0m, weights = \u001b[1m[0.  0.5 0.5]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:21:06] Blending: no score update. Terminated\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.blend:Blending: no score update. Terminated\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:21:06] \u001b[1mAutoml preset training completed in 204.40 seconds\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.presets.base:\u001b[1mAutoml preset training completed in 204.40 seconds\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:21:06] Model description:\n",
            "Final prediction for new objects (level 0) = \n",
            "\t 0.50000 * (2 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
            "\t 0.50000 * (3 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.automl.presets.base:Model description:\n",
            "Final prediction for new objects (level 0) = \n",
            "\t 0.50000 * (2 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
            "\t 0.50000 * (3 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:21:06] ==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightautoml.addons.utilization.utilization:==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4min 57s, sys: 6.45 s, total: 5min 4s\n",
            "Wall time: 3min 24s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "725ede38",
        "papermill": {
          "duration": 0.189096,
          "end_time": "2022-09-29T21:33:39.855422",
          "exception": false,
          "start_time": "2022-09-29T21:33:39.666326",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Prediction for test data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = automl.predict(test_df)\n",
        "print(f'Prediction for te_data:\\n{test_pred}\\nShape = {test_pred.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e013cW9UDHRy",
        "outputId": "3898b716-af49-433e-a6d9-5d7c924523ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for te_data:\n",
            "array([[0.00656395],\n",
            "       [0.00816865],\n",
            "       [0.00441861],\n",
            "       ...,\n",
            "       [0.00490397],\n",
            "       [0.02964572],\n",
            "       [0.01784633]], dtype=float32)\n",
            "Shape = (19260, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'OOF score: {roc_auc_score(train_df[TARGET_NAME].values, oof_pred.data[:, 0])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "GK6ZUCKvDOIb",
        "outputId": "41d15ca9-3311-4537-c268-8c0c09370391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input contains NaN.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-fe91d3cb6f76>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'OOF score: {roc_auc_score(train_df[TARGET_NAME].values, oof_pred.data[:, 0])}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     if y_type == \"multiclass\" or (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oof_pred.data[:, 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWArVxMoD7KV",
        "outputId": "a3c17853-5cc5-49c4-baa7-15c150c5c404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.44725886,        nan, 0.00581809, ..., 0.02756848,        nan,\n",
              "       0.00322664], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.count_nonzero(~np.isnan(oof_pred.data[:, 0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41JMCndiEK_L",
        "outputId": "55582d3b-b52e-4356-9095-1a709ec04766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26964"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.count_nonzero(np.isnan(oof_pred.data[:, 0])) # NaNs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9NHDGPYEE1f",
        "outputId": "dd966162-fcf3-4a0c-d826-98160ac07718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17975"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.count_nonzero(np.isnan(test_pred.data[:, 0])) # NaNs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jfrroJeEKh1",
        "outputId": "aba5ff2f-8b68-47b5-bcc2-d5ca086a7703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Test score: {roc_auc_score(test_df[TARGET_NAME].values, test_pred.data[:, 0])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--iHlgoxEoxI",
        "outputId": "a1b4f4c0-f890-42b7-8215-4a90a809fefa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score: 0.885788495114911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_auc = roc_auc_score(test_df[TARGET_NAME].values, test_pred.data[:, 0])\n",
        "test_acc = accuracy_score(test_df[TARGET_NAME].values, test_pred.data[:, 0] > 0.5)\n",
        "\n",
        "print(\"RFC test metrics: AUC={}, acc={}\".format(test_auc, test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE8aMCSYL88j",
        "outputId": "e292a044-fc9d-4382-90fb-6778dcc4b013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RFC test metrics: AUC=0.885788495114911, acc=0.9280373831775701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38da6384",
        "papermill": {
          "duration": 0.185969,
          "end_time": "2022-09-29T21:34:26.553743",
          "exception": false,
          "start_time": "2022-09-29T21:34:26.367774",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Model analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(automl.create_model_str_desc())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odzDglD1DRJi",
        "outputId": "4c91d675-ea3e-487e-a6e1-650af90af28c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final prediction for new objects = \n",
            "\t1.00000 * 1 averaged models with config = \"conf_0_sel_type_0.yml\" and different CV random_states. Their structures: \n",
            "\n",
            "\t    Model #0.\n",
            "\t\t================================================================================\n",
            "\t\tFinal prediction for new objects (level 0) = \n",
            "\t\t\t 0.11268 * (4 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
            "\t\t\t 0.52931 * (2 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
            "\t\t\t 0.35802 * (3 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) \n",
            "\t\t================================================================================\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7155477b",
        "papermill": {
          "duration": 0.186299,
          "end_time": "2022-09-29T21:34:28.428788",
          "exception": false,
          "start_time": "2022-09-29T21:34:28.242489",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "\n",
        "For feature importances calculation we have 2 different methods in LightAutoML:\n",
        "- Fast (`fast`) - this method uses feature importances from feature selector LGBM model inside LightAutoML. It works extremely fast and almost always (almost because of situations, when feature selection is turned off or selector was removed from the final models with all GBM models). There is no need to use new labelled data.\n",
        "- Accurate (`accurate`) - this method calculate *features permutation importances* for the whole LightAutoML model based on the **new labelled data**. It always works but can take a lot of time to finish (depending on the model structure, new labelled dataset size etc.)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Fast feature importances calculation\n",
        "fast_fi = automl.get_feature_scores('fast')\n",
        "fast_fi.set_index('Feature')['Importance'].plot.bar(figsize = (30, 10), grid = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "vPyuYPp5DVRN",
        "outputId": "6b95e553-369c-445c-94c3-84a19de72612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'set_index'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'set_index'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fast_fi"
      ],
      "metadata": {
        "id": "184Z38tWE6s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ds-Besx0E-pq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}